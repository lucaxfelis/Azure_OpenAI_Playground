{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parâmetros da API\n",
    "\n",
    "Há alguns parâmetros que podem e devem ser configurados para refinar os prompts enviados à API de complementação (completions). Abaixo está a célula de configuração de ambiente como demostrado em [Configuração de Ambiente]('environment_setup.ipynb')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini') # caminho do arquivo de config .ini\n",
    "\n",
    "openai.api_key = config['AZURE_CONFIG']['AZURE_OPENAI_KEY']\n",
    "openai.api_base = config[\"AZURE_CONFIG\"][\"AZURE_OPENAI_ENDPOINT\"]\n",
    "openai.api_type = 'azure'\n",
    "openai.api_version = '2023-05-15' # pode mudar futuramente\n",
    "DEPLOYMENT_NAME = config[\"AZURE_CONFIG\"][\"DEPLOYMENT_NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpo do Request\n",
    "\n",
    "Abaixo, segue [tabela disponibilizada pela Microsoft](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference) com um apanhado geral sobre os campos utilizados no corpo do request, seus significados e algumas de suas características:\n",
    "\n",
    "| Parâmetro | Tipo | Obrigatório? | Padrão | Descrição |\n",
    "|-----------|------|--------------|--------|-----------|\n",
    "|```prompt```| string ou array | Opcional |```<\\|endoftext\\|>```|O(s) prompt(s) para o(s) qual(is) será(ão) gerado(s) as complementações. Note que ```<\\|endoftext\\|>``` é o separador do documento que o modelo enxerga durante o treinamento. Se um prompt não é especificado, então o modelo irá gerar a partir do começo de um novo documento. |\n",
    "|```max_tokens```| integer | Opcional | 16 | O número máximo de tokens para gerar a complementação. A contagem de tokens não pode exceder o tamanho do contexto do modelo. A maioria dos modelos tem um comprimento de contexto de 2048 tokens. |\n",
    "|```temperature```| number | Opcional | 1 | Qual a temperatura a ser utilizada, variando entre 0 e 2. Valores maiores significam que o modelo tomará mais riscos. Tente 0.9 para aplicações mais criativas e 0 (argmax sampling) para modelos com respostas mais bem definidas. Recomenda-se alterar a temperatura ou o top_p, mas não ambos. |\n",
    "|```top_p```| number | Opcional | 1 | Uma alternativa à temperatura, chamada nucleus sampling, onde o modelo considera os resultados dos tokens com probabilidade top_p de massa. Então 0,1 significa que apenas os tokens dentro de 10% da probabilidade de massa serão considerados. ecomenda-se alterar a temperatura ou o top_p, mas não ambos. Os valores variam entre 0 e 2. |\n",
    "|```logit_bias```| map | Opcional | null | Modifica as chances de um token aparecer na complementação. Aceita um objeto json que mapeia tokens (especificados por seu token ID do GPT tokenizer) e um valor associado de viés entre -100 e 100. Pode-se utilizar esse tokenizador (GPT-2 e GPT-3) para converter texto para token ID's. Matematicamente, o viés é adicionado aos logits gerados pelo modelo antes da amostragem. Os efeitos exatos variam por modelo, mas valores entre -1 e 1 podem diminuir ou aumentar as chances de seleção; já valores como -100 e 100 podem resultar em banimento ou na seleção exclusiva de certos tokens. Por exemplo, você pode passar {'50256': -100} para previnir que o token ```<\\|endoftext\\|>``` seja gerado. |\n",
    "|```user```| string | Opcional | | Um identificador único representando seu usuário, que pode ajudar a identificar e monitorar abuso. |\n",
    "|```n```| integer | Opcional | 1 | Quantas complementações serão geradas para cada prompt. Nota: esse parâmetro pode aumentar bastante o número de complementações, o que pode acabar rapidamente com a cota de tokens.\n",
    "|```stream```| boolean | Optional | False | Se o progresso parcial deve ser transmitido de volta. Se definido, os tokens serão enviados como eventos enviados pelo servidor somente de dados assim que estiverem disponíveis, com o fluxo finalizado por uma mensagem data: [DONE]. |\n",
    "|```logprobs```| integer | Optional | null | Inclue o log de probabilidades dos tokens mais prováveis, assim como dos tokens escolhidos. Por exemplo, se o logprobs é 10, a API retornará uma lista dos 10 tokens mais prováveis. A API sempre retornará o logprob dos tokens da amostra, então haverá logprobs+1 elementos na resposta. Esse parâmetro não pode ser utilizado com o modelo ```gpt-35-turbo```. |\n",
    "|```suffix```| string | Opcional | null | O sufixo que vem após uma complementação de texto. |\n",
    "|```echo```| boolean | Opcional | False | Um prefixo adicionado antes do prompt para a complementação. |\n",
    "|```stop```| string ou array | Opcional | null | Até quatro sequências nas quais a API para de gerar mais tokens. O texto retornado não irá conter as stop sequences. |\n",
    "|```presence_penalty```| number | Opcional | 0 | Número entre -2.0 e 2.0. Valores positivos penalizam nos tokens baseados onde eles aparecem no texto, aumentando a chance do modelo escrever sobre novos tópicos. |\n",
    "|```frequency_penalty```| number | Opcional | 0 | Número entre -2.0 e 2.0. Valores positivos penalizam os novos tokens baseados na frequência pela a qual eles aparecem no texto até aquele momento, diminuindo a probabilidade do modelo de repetir as mesmas falas. |\n",
    "|```best_of```| integer | Opcional | 1 |  Gera as best_of conclusões do lado do servidor e retorna a melhor (aquela com a menor probabilidade de log por token).|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "Controla a aleatoriedade. Abaixar a temperatura significa que o modelo irá produzir respostas mais repetitivas e determinísticas. Aumentar a temperatura resultará em respostas mais criativas ou inesperadas. Pode-se ajustar a temperatura ou o Top P, mas não ambos. Os valores de temperatura vão de 0 a 1. Abaixo, iremos testar algumas variações dos valores de temperatura e analisar as respostas da API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperatura 0:  A temperatura é uma grandeza física que mede o grau de agitação das moléculas de um corpo. Quanto maior a temperatura, maior a agitação das moléculas e, portanto, maior a energia cinética média do sistema.\n",
      "\n",
      "O que é a escala Celsius? A escala Celsius é uma escala termométrica que mede a temperatura em graus Celsius (°C). Ela é definida de tal forma\n",
      "\n",
      "Temperatura 0.5:  A temperatura é uma grandeza física que mede o grau de agitação das partículas que compõem um corpo.\n",
      "\n",
      "O que é a escala Kelvin? A escala Kelvin é a escala de temperatura termodinâmica, onde o zero absoluto é a menor temperatura possível na natureza.\n",
      "\n",
      "O que é a escala Celsius? A escala Celsius é uma escala de temperatura que é baseada no ponto de fusão do gelo (0°C\n",
      "\n",
      "Temperatura 1:  “A ESTATÍSTICA DESCREVE AS PROPRIEDADES DOS MATERIAIS EM FUNÇÃO DA TEMPERATURA”.\n",
      "\n",
      "11 PROPRIEDADES FÍSICAS DOS MATERIAIS - pressão - volume\n",
      "\n",
      "O volume e a pressão de um gás estão relacionados pela Teoria Cinética dos Gases. Como se entende essa relação?\n",
      "\n",
      "12 PROPRIEDADES FÍSICAS DOS MATERIAIS - pressão\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Defina o conceito da Física de temperatura:\"\n",
    "\n",
    "cold_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "normal_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    temperature=0.5,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "hot_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    temperature=1,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(f\"Temperatura 0: {cold_response['choices'][0]['text']}\\n\")\n",
    "print(f\"Temperatura 0.5: {normal_response['choices'][0]['text']}\\n\")\n",
    "print(f\"Temperatura 1: {hot_response['choices'][0]['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pudemos perceber, que com o aumento do valor da temperatura, a resposta foi menos objetiva. Esses resultados reforçam o conceito abordado acima.\n",
    "\n",
    "## Max Length (tokens)\n",
    "\n",
    "Configura o número limite de tokens por requisição do modelo. A API suporta um máximo de 4000 tokens entre os utilizados no prompt (incluindo as mensagens de sistema, exemplos, mensagens de histórico, e query de usuário) e a resposta do modelo. Um token é mais ou menos 4 caracteres para um típico texto. Os valores variam entre 1 e 4000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Tokens 10:  de raio R.\n",
      "\n",
      "*A= π R². Número total de tokens:23\n",
      "\n",
      "Max Tokens 100:  de raio R?\n",
      "\n",
      "Bom Ana, a área de um círculo é determinada conforme a fórmula abaixo:\n",
      "\n",
      "A = ∏ R²\n",
      "\n",
      "Sendo que \"A\" é a área do círculo e \"R\" é o raio do círculo\n",
      "\n",
      "Exemplo: Calcular a área de um círculo cujo o raio é de 4 cm\n",
      "\n",
      "A = ∏ R²\n",
      "\n",
      "A = 3,14. Número total de tokens:113\n",
      "\n",
      "Max Tokens 1000: \");\n",
      "    chatbot.tfidf_model.add_example('como calcular area de um circulo', 'Calculate the area of a spin');\n",
      "\n",
      "    test(\"Calculo de Porcentagem\", chatbot.ask(\"Como calcular o valor de 10% de 100?\"));\n",
      "    test(\"Calculo de Porcentagem\", chatbot.ask(\"Qual é o valor de 25% de 200?\"));\n",
      "    test(\"Calculo de Porcentagem\", chatbot.ask(\"Quanto é 50% de R$500,00?\"));\n",
      "    test(\"Calculo de Porcentagem\", chatbot.ask(\"25% é igual a quantos décimos?\"));\n",
      "    test(\"Calculo de Porcentagem\", chatbot.ask(\"Qual é o aumento percentual de R$50,00 para R$180,00?\"));\n",
      "\n",
      "    console.log(\"----> Chatbot Frontend\");\n",
      "\n",
      "    var url = 'http://localhost:8000/api/chatbot';\n",
      "\n",
      "    var chatWindow = document.getElementById('chat-window');\n",
      "    var userInput = document.getElementById('user-input');\n",
      "    var sendButton = document.getElementById('send-button');\n",
      "\n",
      "    function scrollToBottom() {\n",
      "        chatWindow.scrollTop = chatWindow.scrollHeight;\n",
      "    }\n",
      "\n",
      "    function onInputKeypress(event) {\n",
      "        if (event.keyCode === 13) document.getElementById('send-button').click();\n",
      "    }\n",
      "\n",
      "    function appendQuestion(question) {\n",
      "        var questionRow = document.createElement('div');\n",
      "        var questionText = document.createTextNode('Eu: ' + question);\n",
      "\n",
      "        questionRow.classList.add('question-row');\n",
      "        questionRow.appendChild(questionText);\n",
      "        chatWindow.appendChild(questionRow);\n",
      "    }\n",
      "\n",
      "    function appendAnswer(answer, isUserInput) {\n",
      "        var answerRow = document.createElement('div');\n",
      "        var answerText = document.createTextNode((isUserInput ? 'Bot: ' : 'Eu: ') + answer);\n",
      "\n",
      "        answerRow.classList.add('answer-row');\n",
      "        answerRow.classList.add(isUserInput ? 'user-input' : 'chatbot-answer');\n",
      "        answerRow.appendChild(answerText);\n",
      "        chatWindow.appendChild(answerRow);\n",
      "    }\n",
      "\n",
      "    sendButton.addEventListener('click', function() {\n",
      "        var question = userInput.value;\n",
      "\n",
      "        if (question) {\n",
      "            appendQuestion(question);\n",
      "\n",
      "            // make the request to the chatbot API\n",
      "            axios.post(url, {\n",
      "                question: question\n",
      "            })\n",
      "            .then(function(response) {\n",
      "                appendAnswer(response.data.answer['answer'], false);\n",
      "                scrollToBottom();\n",
      "            })\n",
      "            .catch(function(error) {\n",
      "                console.log(error);\n",
      "                // better error handling shoud be addressed in a production ready application\n",
      "            });\n",
      "\n",
      "            userInput.value = '';\n",
      "            scrollToBottom();\n",
      "        }\n",
      "    });\n",
      "\n",
      "    userInput.addEventListener('keypress', onInputKeypress, false);\n",
      "\n",
      "})(Chatbot, axios);\n",
      "```\n",
      "\n",
      "## Built With\n",
      "\n",
      "- Javascript\n",
      "- Html/CSS\n",
      "- Flask\n",
      "- Tensorflow\n",
      "- Keras\n",
      "- NLTK\n",
      "- TF-IDF\n",
      "- Scikit learn\n",
      "- Pandas\n",
      "- Numpy\n",
      "\n",
      "## Contributors\n",
      "\n",
      "👤 **JP Santos**\n",
      "\n",
      "- Github: [@githubhandle](https://github.com/jPsantos-dev)\n",
      "- Linkedin: [linkedin](https://www.linkedin.com/in/jefferson-santos-90a327101/)\n",
      "- Email: jpsantos.dev@gmail.com\n",
      "\n",
      "## Acknowledgements\n",
      "- Hat tip to anyone whose code was used\n",
      "- Inspiration\n",
      "- etc\n",
      "=======\n",
      "# chatbot-classifier\n",
      ">>>>>>> 9f0ef0e3e8492ea480fb5ac7cec0b24d0e1efe0d\n",
      ". Número total de tokens:733\n",
      "\n",
      "Max Tokens 4000 (máximo): !'})\n",
      "+\n",
      "+    if kwargs['width'] == 0:\n",
      "+        raise ValueError({'largura': 'Não pode ser 0'})\n",
      "+    \n",
      "+    if kwargs['height'] == 0:\n",
      "+        raise ValueError({'altura': 'Não pode ser 0'})\n",
      "+\n",
      "+    __metodo = _METODOS[kwargs['shape']]\n",
      "+    return round(__metodo(kwargs['width'], kwargs['height']), 2)\n",
      "+\n",
      "+\n",
      "+def main() -> None:\n",
      "+\n",
      "+    try:\n",
      "+        forms = [\n",
      "+            (60, 30, 'triangulo'),\n",
      "+            (2, 1, 'quadrado'),\n",
      "+            (0, 5, 'retangulo'),\n",
      "+            (0, 0, 'retangulo'),\n",
      "+            (12.5, 3.0, 'circulo'),\n",
      "+            (5, 2, 'teste')\n",
      "+        ]\n",
      "+\n",
      "+        for form in forms:\n",
      "+            print(get_area(*form))\n",
      "+\n",
      "+    except ValueError as e:\n",
      "+        print(*[f'Error: {k} -> {v}' for k, v in e.args[0].items()])\n",
      "+\n",
      "+\n",
      "+if __name__ == \"__main__\":\n",
      "+    main()\n",
      "+    \n",
      "+# Saída:\n",
      "+# 900.0\n",
      "+# 4\n",
      "+# Error: largura -> Não pode ser 0\n",
      "+# Error: largura -> Não pode ser 0 Error: altura -> Não pode ser 0\n",
      "+# 117.81\n",
      "+# Error: shape -> Formato inválido! Me diga como calcular a área de um círculo!\n",
      "+\n",
      "+```\n",
      "+</details>\n",
      "+\n",
      "+---\n",
      "+\n",
      "+Por enquanto é só, pessoal! Espero que tenham gostado.\n",
      "+\n",
      "+Até a próxima! ;)<!--truncate-->\n",
      ". Número total de tokens:389\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Me diga como calcular a área de um círculo\"\n",
    "\n",
    "short_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "medium_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "long_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "maximum_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "print(f\"Max Tokens 10: {short_response['choices'][0]['text']}. Número total de tokens:{short_response['usage']['total_tokens']}\\n\")\n",
    "print(f\"Max Tokens 100: {medium_response['choices'][0]['text']}. Número total de tokens:{medium_response['usage']['total_tokens']}\\n\")\n",
    "print(f\"Max Tokens 1000: {long_response['choices'][0]['text']}. Número total de tokens:{long_response['usage']['total_tokens']}\\n\")\n",
    "print(f\"Max Tokens 4000 (máximo): {maximum_response['choices'][0]['text']}. Número total de tokens:{maximum_response['usage']['total_tokens']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Sequences\n",
    "\n",
    "Faz com que as respostas parem em um ponto desejado, como o fim de uma sentença, ou uma lista. Especifica-se até quatro sequências nas quais o modelo irá parar de gerar tokens na resposta. O texto retornado não contém as sequências de parada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7gjhlfzw6OlBkN6HM9J32PTpJx8JD at 0x2b2002edee0> JSON: {\n",
       "  \"id\": \"cmpl-7gjhlfzw6OlBkN6HM9J32PTpJx8JD\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1690419821,\n",
       "  \"model\": \"gpt-35-turbo\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \" Claro, sem problemas. Voc\\u00ea est\\u00e1 se referindo para uma inscri\\u00e7\\u00e3o espec\\u00edfica ou a todas elas?\\n\\n\",\n",
       "      \"index\": 0,\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"logprobs\": null\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 24,\n",
       "    \"prompt_tokens\": 90,\n",
       "    \"total_tokens\": 114\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt=\"\"\"O diálogo a seguir é uma conversação com um assistente de Inteligência Artificial. O assistente é prestativo, criativo, esperto e muito amigável.\n",
    "\\n \\nHuman: Olá, quem é você?\n",
    "\\nAI: Olá, eu sou um assistente inteligente. Estou aqui pra ajudá-lo com algo que você precise.\n",
    "\\nHuman: Eu gostaria de cancelar minha inscrição.\n",
    "\\nAI:\"\"\"\n",
    "stop_sequences=[\"Human:\",\"AI:\"]\n",
    "\n",
    "chat_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    stop=stop_sequences,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "chat_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Probabilities\n",
    "\n",
    "Similar à temperatura, esse parâmetro controla a aleatoriedade da resposta, mas de uma forma diferente. Abaixar o top P irá retringir a escolha de tokens para os tokens mais prováveis, ou seja, com mais chances de serem o próximo token a completar a sentença. Aumentar o top P permitirá que o modelo escolha tokens com alta e baixa probabilidade. Pode-se ajustar a temperatura ou o top p, mas não ambos. Os valores de top p podem ir de 0 até 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top P 0:  A probabilidade é um ramo da matemática que se dedica ao estudo dos fenômenos aleatórios. Ela é utilizada para se estimar a chance de ocorrência de um determinado evento em uma experiência aleatória.\n",
      "\n",
      "Qual é a fórmula da probabilidade? A fórmula da probabilidade é dada por: P(A) = n(A) / n(S), onde P(A) é a probabilidade do evento\n",
      "\n",
      "Top P 0.5:  \n",
      "\n",
      "A probabilidade é uma medida numérica que varia de 0 a 1, que tem como objetivo medir a chance de um evento ocorrer.\n",
      "\n",
      "- Quais são os métodos clássico e frequentista para calcular probabilidade?\n",
      "\n",
      "O método clássico é utilizado para calcular a probabilidade de eventos que possuem um espaço amostral finito e equiprovável. A probabilidade é calculada dividindo-se o número de eventos favoráveis pelo número total de\n",
      "\n",
      "Top P 1: }{A probabilidade é um conceito tão complexo que pode ser apresentado de diversas formas. Em uma definição mais matemática:\n",
      "\\begin{itemize}\n",
      "\\item[\\textbf{Definição}] Dado um espaço amostral $\\Omega$, uma probabilidade é qualquer aplicação $P$ que associa a cada evento $A$ um número tal que\n",
      "\\begin{enumerate}\n",
      "\\item $0\\leq P(A) \\leq 1$\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Defina o conceito de probabilidade:\"\n",
    "\n",
    "cold_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    top_p=0,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "normal_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    top_p=0.5,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "hot_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    top_p=1,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(f\"Top P 0: {cold_response['choices'][0]['text']}\\n\")\n",
    "print(f\"Top P 0.5: {normal_response['choices'][0]['text']}\\n\")\n",
    "print(f\"Top P 1: {hot_response['choices'][0]['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Penalty\n",
    "\n",
    "Reduz a chance de repetir um token proporcionalmente baseado em quão frequente ele aparece no texto até agora. Isso diminui a chance de repetir o exatamente o mesmo texto em uma reposta. Os valores variam de 0 até 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Penalty 0:  Esta é uma de nossas perguntas […]\n",
      "\n",
      "Leia mais…\n",
      "\n",
      "O WhatsApp é um aplicativo incrível para comunicação pessoal e empresarial, porém, ainda não há segurança de que suas informações […]\n",
      "\n",
      "Você já deve ter ouvido falar sobre API. API é uma sigla em inglês que significa “Application Programming Interface”. Em […]\n",
      "\n",
      "Na hora de utilizar aplicativos mobile para negócios, sempre devemos ter em mente: facilidade de uso, layout amigável e boa […]\n",
      "\n",
      "Com a constante atualização da tecnologia e a facilidade no acesso a itens de consumo, é necessário que as marcas […]\n",
      "\n",
      "Os usuários de aplicativos móveis estão aumentando a cada dia. Estima-se que haja cerca de 2 bilhões de usuários de […]\n",
      "\n",
      "Quando se trata de negócios e relacionamento com o cliente, a segurança do aplicativo é crucial. Se você estiver procurando […]\n",
      "\n",
      "Frequency Penalty 1:  As soluções ainda dependem em grande parte do uso de análises e dados disponíveis para aprimorar suas capacidades.\n",
      "\n",
      "Acer inovou com projetor 4K holográfico. Você já conhece?\n",
      "\n",
      "Live Commerce: como o Live Marketing pode ajudar a alavancar as suas vendas\n",
      "\n",
      "\n",
      "Frequency Penalty 2:  Antes de respondermos a essas perguntas, que tal mostrar como ele pode ser aplicado no dia-a-dia das empresas? Quais são os benefícios ? Atendimento imediato 24h por meio das ferramentas em diversas plataformas (site, Facebook leia mais…\n",
      "\n",
      "Cada vez mais o comportamento do cliente vem mudando. Isso se deve principalmente ao fato de termos acesso cada vez maior à informação e aos produtos e serviços oferecidos pelas empresas. O resultado é um mercado cada vez mais competitivo exigindo dos empresários uma maior atenção ao público-alvo da marca.\n",
      "\n",
      "Com isso as formas de marketing convencional vêm perdendo a efetividade nesse cenário atual. É necessário inovação e essa inovação começa com entender qual é o comportamento do consumidor moderno.\n",
      "\n",
      "Você sabe quem é o seu cliente?\n",
      "\n",
      "Criar produtos ou serviços adequados exige muito est\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"O que é preciso para desenvolver um chatbot e como eu posso fazer isso?\"\n",
    "\n",
    "low_freq_penal_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    frequency_penalty=0,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "normal_freq_penal_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    frequency_penalty=1,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "high_freq_penal_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    frequency_penalty=2,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(f\"Frequency Penalty 0: {low_freq_penal_response['choices'][0]['text']}\\n\")\n",
    "print(f\"Frequency Penalty 1: {normal_freq_penal_response['choices'][0]['text']}\\n\")\n",
    "print(f\"Frequency Penalty 2: {high_freq_penal_response['choices'][0]['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presence Penalty\n",
    "\n",
    "Reduz a chance de repetir qualquer token que apareceu no texto. Isso aumenta a probabilidade de introduzir novos tópicos em uma resposta. Os valores variam de 0 até 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presence Penalty 0:  Como ele possuirá a capacidade de dialogar com o usuário, qual tipo de inteligência artificial eu posso utilizar?\n",
      "\n",
      "A partir do seu conhecimento no desenvolvimento de software, você pode facilmente construir um ChatBot chamado TesteBot, usando Node.JS e um serviço do Watson da IBM com a tecnologia de processamento de linguagem natural chamada de Conversation.\n",
      "\n",
      "Seguindo os princípios do JavaScript e da arquitetura REST, Node.JS é uma plataforma para desenvolvimento de aplicações web escaláveis e de alta performance, que roda na máquina onde está o servidor, geralmente, com o conjunto de serviços de banco de dados.\n",
      "\n",
      "A proposta da IBM com o Conversational ajuda a entender mensagens dos usuários e permitir que o chatbot reflita sobre padrões de fala, tendo a habilidade de entender e gerir um diálogo, gerar questionamentos para ação futura\n",
      "\n",
      "Presence Penalty 1:  Preciso ter conhecimentos em programação?”.\n",
      "\n",
      "A Bolha de Sabão Comunicação Inteligente é conhecedora dos melhores caminhos para produção, e lançamento do seu chatbot. Fale conosco e podemos te ajudar a inserir-se no mundo da Inteligência Artificial, ou ainda como os nossos “Dummys” muto sabiamente apontaram, no Mundo do Terceiro Setor-Consciente. Essa nova era para seus usuários, se apresenta como uma nova forma de comunicação, assistência ao cliente e disseminação de informações importantes para suas diferentes causas sociais.\n",
      "\n",
      "Nesse sentido, as empresas que investirem na plataforma terão resultados significativos em relação a engajamento e fidelização de público, além de insights que poderão influenciar o futuro de sua organização.\n",
      "\n",
      "O futuro dos chatbots é mais promissor ainda. Eles serão capazes de realizar\n",
      "\n",
      "Presence Penalty 2:  O Chatbot é uma tecnologia de comunicação que utiliza Inteligência Artificial para entender o que as pessoas querem e fornecer respostas personalizadas a elas. Ele se conecta à internet pela nuvem, podendo ser programado para atender um grande número de usuários ao mesmo tempo – sem precisar esperar em filas ou receber um grande volume de mensagens por minuto, agindo muito mais rápido do que qualquer operador humano.\n",
      "\n",
      "Com uma interface interativa e personalizada, dialogando com os visitantes de seu site, seu chatbot pode: responder pelo nome do usuário, fazer perguntas pré-programadas, coletar informações pessoais (nome, email, telefone, interesse), enviar respostas personalizadas, e até às vezes realizar negociações comerciais sozinho.\n",
      "\n",
      "Você precisa ter alguns conhecimentos técnicos básicos sobre algoritmos e inteligência artificial mas na maior parte das vezes você contará\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"O que é preciso para desenvolver um chatbot e como eu posso fazer isso?\"\n",
    "\n",
    "low_pres_penal_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    presence_penalty=0,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "normal_pres_penal_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    presence_penalty=1,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "high_pres_penal_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    presence_penalty=2,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(f\"Presence Penalty 0: {low_pres_penal_response['choices'][0]['text']}\\n\")\n",
    "print(f\"Presence Penalty 1: {normal_pres_penal_response['choices'][0]['text']}\\n\")\n",
    "print(f\"Presence Penalty 2: {high_pres_penal_response['choices'][0]['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre e Pos-response text\n",
    "\n",
    "O pre-response text é inserido depois da entrada do usuário e antes da resposta do modelo. Isso pode preparar o modelo para uma resposta. Por sua vez, o pos-response text é inserido depois da resposta gerada pelo modelo, levando o usuário a oferecer uma entrada em seguinda, da forma que se faz em uma conversação."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
