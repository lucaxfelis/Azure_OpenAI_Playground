{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Par√¢metros da API\n",
    "\n",
    "H√° alguns par√¢metros que podem e devem ser configurados para refinar os prompts enviados √† API de complementa√ß√£o (completions). Abaixo est√° a c√©lula de configura√ß√£o de ambiente como demostrado em [Configura√ß√£o de Ambiente]('environment_setup.ipynb')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini') # caminho do arquivo de config .ini\n",
    "\n",
    "openai.api_key = config['AZURE_CONFIG']['AZURE_OPENAI_KEY']\n",
    "openai.api_base = config[\"AZURE_CONFIG\"][\"AZURE_OPENAI_ENDPOINT\"]\n",
    "openai.api_type = 'azure'\n",
    "openai.api_version = '2023-05-15' # pode mudar futuramente\n",
    "DEPLOYMENT_NAME = config[\"AZURE_CONFIG\"][\"DEPLOYMENT_NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpo do Request\n",
    "\n",
    "Abaixo, segue [tabela disponibilizada pela Microsoft](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference) com um apanhado geral sobre os campos utilizados no corpo do request, seus significados e algumas de suas caracter√≠sticas:\n",
    "\n",
    "| Par√¢metro | Tipo | Obrigat√≥rio? | Padr√£o | Descri√ß√£o |\n",
    "|-----------|------|--------------|--------|-----------|\n",
    "|```prompt```| string ou array | Opcional |```<\\|endoftext\\|>```|O(s) prompt(s) para o(s) qual(is) ser√°(√£o) gerado(s) as complementa√ß√µes. Note que ```<\\|endoftext\\|>``` √© o separador do documento que o modelo enxerga durante o treinamento. Se um prompt n√£o √© especificado, ent√£o o modelo ir√° gerar a partir do come√ßo de um novo documento. |\n",
    "|```max_tokens```| integer | Opcional | 16 | O n√∫mero m√°ximo de tokens para gerar a complementa√ß√£o. A contagem de tokens n√£o pode exceder o tamanho do contexto do modelo. A maioria dos modelos tem um comprimento de contexto de 2048 tokens. |\n",
    "|```temperature```| number | Opcional | 1 | Qual a temperatura a ser utilizada, variando entre 0 e 2. Valores maiores significam que o modelo tomar√° mais riscos. Tente 0.9 para aplica√ß√µes mais criativas e 0 (argmax sampling) para modelos com respostas mais bem definidas. Recomenda-se alterar a temperatura ou o top_p, mas n√£o ambos. |\n",
    "|```top_p```| number | Opcional | 1 | Uma alternativa √† temperatura, chamada nucleus sampling, onde o modelo considera os resultados dos tokens com probabilidade top_p de massa. Ent√£o 0,1 significa que apenas os tokens dentro de 10% da probabilidade de massa ser√£o considerados. ecomenda-se alterar a temperatura ou o top_p, mas n√£o ambos. Os valores variam entre 0 e 2. |\n",
    "|```logit_bias```| map | Opcional | null | Modifica as chances de um token aparecer na complementa√ß√£o. Aceita um objeto json que mapeia tokens (especificados por seu token ID do GPT tokenizer) e um valor associado de vi√©s entre -100 e 100. Pode-se utilizar esse tokenizador (GPT-2 e GPT-3) para converter texto para token ID's. Matematicamente, o vi√©s √© adicionado aos logits gerados pelo modelo antes da amostragem. Os efeitos exatos variam por modelo, mas valores entre -1 e 1 podem diminuir ou aumentar as chances de sele√ß√£o; j√° valores como -100 e 100 podem resultar em banimento ou na sele√ß√£o exclusiva de certos tokens. Por exemplo, voc√™ pode passar {'50256': -100} para previnir que o token ```<\\|endoftext\\|>``` seja gerado. |\n",
    "|```user```| string | Opcional | | Um identificador √∫nico representando seu usu√°rio, que pode ajudar a identificar e monitorar abuso. |\n",
    "|```n```| integer | Opcional | 1 | Quantas complementa√ß√µes ser√£o geradas para cada prompt. Nota: esse par√¢metro pode aumentar bastante o n√∫mero de complementa√ß√µes, o que pode acabar rapidamente com a cota de tokens.\n",
    "|```stream```| boolean | Optional | False | Se o progresso parcial deve ser transmitido de volta. Se definido, os tokens ser√£o enviados como eventos enviados pelo servidor somente de dados assim que estiverem dispon√≠veis, com o fluxo finalizado por uma mensagem data: [DONE]. |\n",
    "|```logprobs```| integer | Optional | null | Inclue o log de probabilidades dos tokens mais prov√°veis, assim como dos tokens escolhidos. Por exemplo, se o logprobs √© 10, a API retornar√° uma lista dos 10 tokens mais prov√°veis. A API sempre retornar√° o logprob dos tokens da amostra, ent√£o haver√° logprobs+1 elementos na resposta. Esse par√¢metro n√£o pode ser utilizado com o modelo ```gpt-35-turbo```. |\n",
    "|```suffix```| string | Opcional | null | O sufixo que vem ap√≥s uma complementa√ß√£o de texto. |\n",
    "|```echo```| boolean | Opcional | False | Um prefixo adicionado antes do prompt para a complementa√ß√£o. |\n",
    "|```stop```| string ou array | Opcional | null | At√© quatro sequ√™ncias nas quais a API para de gerar mais tokens. O texto retornado n√£o ir√° conter as stop sequences. |\n",
    "|```presence_penalty```| number | Opcional | 0 | N√∫mero entre -2.0 e 2.0. Valores positivos penalizam nos tokens baseados onde eles aparecem no texto, aumentando a chance do modelo escrever sobre novos t√≥picos. |\n",
    "|```frequency_penalty```| number | Opcional | 0 | N√∫mero entre -2.0 e 2.0. Valores positivos penalizam os novos tokens baseados na frequ√™ncia pela a qual eles aparecem no texto at√© aquele momento, diminuindo a probabilidade do modelo de repetir as mesmas falas. |\n",
    "|```best_of```| integer | Opcional | 1 |  Gera as best_of conclus√µes do lado do servidor e retorna a melhor (aquela com a menor probabilidade de log por token).|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "Controla a aleatoriedade. Abaixar a temperatura significa que o modelo ir√° produzir respostas mais repetitivas e determin√≠sticas. Aumentar a temperatura resultar√° em respostas mais criativas ou inesperadas. Pode-se ajustar a temperatura ou o Top P, mas n√£o ambos. Os valores de temperatura v√£o de 0 a 1. Abaixo, iremos testar algumas varia√ß√µes dos valores de temperatura e analisar as respostas da API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperatura 0:  A temperatura √© uma grandeza f√≠sica que mede o grau de agita√ß√£o das mol√©culas de um corpo. Quanto maior a temperatura, maior a agita√ß√£o das mol√©culas e, portanto, maior a energia cin√©tica m√©dia do sistema.\n",
      "\n",
      "O que √© a escala Celsius? A escala Celsius √© uma escala termom√©trica que mede a temperatura em graus Celsius (¬∞C). Ela √© definida de tal forma\n",
      "\n",
      "Temperatura 0.5:  A temperatura √© uma grandeza f√≠sica que mede o grau de agita√ß√£o das part√≠culas que comp√µem um corpo.\n",
      "\n",
      "O que √© a escala Kelvin? A escala Kelvin √© a escala de temperatura termodin√¢mica, onde o zero absoluto √© a menor temperatura poss√≠vel na natureza.\n",
      "\n",
      "O que √© a escala Celsius? A escala Celsius √© uma escala de temperatura que √© baseada no ponto de fus√£o do gelo (0¬∞C\n",
      "\n",
      "Temperatura 1:  ‚ÄúA ESTAT√çSTICA DESCREVE AS PROPRIEDADES DOS MATERIAIS EM FUN√á√ÉO DA TEMPERATURA‚Äù.\n",
      "\n",
      "11 PROPRIEDADES F√çSICAS DOS MATERIAIS - press√£o - volume\n",
      "\n",
      "O volume e a press√£o de um g√°s est√£o relacionados pela Teoria Cin√©tica dos Gases. Como se entende essa rela√ß√£o?\n",
      "\n",
      "12 PROPRIEDADES F√çSICAS DOS MATERIAIS - press√£o\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Defina o conceito da F√≠sica de temperatura:\"\n",
    "\n",
    "cold_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "normal_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    temperature=0.5,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "hot_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    temperature=1,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(f\"Temperatura 0: {cold_response['choices'][0]['text']}\\n\")\n",
    "print(f\"Temperatura 0.5: {normal_response['choices'][0]['text']}\\n\")\n",
    "print(f\"Temperatura 1: {hot_response['choices'][0]['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pudemos perceber, que com o aumento do valor da temperatura, a resposta foi menos objetiva. Esses resultados refor√ßam o conceito abordado acima.\n",
    "\n",
    "## Max Length (tokens)\n",
    "\n",
    "Configura o n√∫mero limite de tokens por requisi√ß√£o do modelo. A API suporta um m√°ximo de 4000 tokens entre os utilizados no prompt (incluindo as mensagens de sistema, exemplos, mensagens de hist√≥rico, e query de usu√°rio) e a resposta do modelo. Um token √© mais ou menos 4 caracteres para um t√≠pico texto. Os valores variam entre 1 e 4000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Tokens 10:  de raio R.\n",
      "\n",
      "*A= œÄ R¬≤. N√∫mero total de tokens:23\n",
      "\n",
      "Max Tokens 100:  de raio R?\n",
      "\n",
      "Bom Ana, a √°rea de um c√≠rculo √© determinada conforme a f√≥rmula abaixo:\n",
      "\n",
      "A = ‚àè R¬≤\n",
      "\n",
      "Sendo que \"A\" √© a √°rea do c√≠rculo e \"R\" √© o raio do c√≠rculo\n",
      "\n",
      "Exemplo: Calcular a √°rea de um c√≠rculo cujo o raio √© de 4 cm\n",
      "\n",
      "A = ‚àè R¬≤\n",
      "\n",
      "A = 3,14. N√∫mero total de tokens:113\n",
      "\n",
      "Max Tokens 1000: \");\n",
      "    chatbot.tfidf_model.add_example('como calcular area de um circulo', 'Calculate the area of a spin');\n",
      "\n",
      "    test(\"Calculo de Porcentagem\", chatbot.ask(\"Como calcular o valor de 10% de 100?\"));\n",
      "    test(\"Calculo de Porcentagem\", chatbot.ask(\"Qual √© o valor de 25% de 200?\"));\n",
      "    test(\"Calculo de Porcentagem\", chatbot.ask(\"Quanto √© 50% de R$500,00?\"));\n",
      "    test(\"Calculo de Porcentagem\", chatbot.ask(\"25% √© igual a quantos d√©cimos?\"));\n",
      "    test(\"Calculo de Porcentagem\", chatbot.ask(\"Qual √© o aumento percentual de R$50,00 para R$180,00?\"));\n",
      "\n",
      "    console.log(\"----> Chatbot Frontend\");\n",
      "\n",
      "    var url = 'http://localhost:8000/api/chatbot';\n",
      "\n",
      "    var chatWindow = document.getElementById('chat-window');\n",
      "    var userInput = document.getElementById('user-input');\n",
      "    var sendButton = document.getElementById('send-button');\n",
      "\n",
      "    function scrollToBottom() {\n",
      "        chatWindow.scrollTop = chatWindow.scrollHeight;\n",
      "    }\n",
      "\n",
      "    function onInputKeypress(event) {\n",
      "        if (event.keyCode === 13) document.getElementById('send-button').click();\n",
      "    }\n",
      "\n",
      "    function appendQuestion(question) {\n",
      "        var questionRow = document.createElement('div');\n",
      "        var questionText = document.createTextNode('Eu: ' + question);\n",
      "\n",
      "        questionRow.classList.add('question-row');\n",
      "        questionRow.appendChild(questionText);\n",
      "        chatWindow.appendChild(questionRow);\n",
      "    }\n",
      "\n",
      "    function appendAnswer(answer, isUserInput) {\n",
      "        var answerRow = document.createElement('div');\n",
      "        var answerText = document.createTextNode((isUserInput ? 'Bot: ' : 'Eu: ') + answer);\n",
      "\n",
      "        answerRow.classList.add('answer-row');\n",
      "        answerRow.classList.add(isUserInput ? 'user-input' : 'chatbot-answer');\n",
      "        answerRow.appendChild(answerText);\n",
      "        chatWindow.appendChild(answerRow);\n",
      "    }\n",
      "\n",
      "    sendButton.addEventListener('click', function() {\n",
      "        var question = userInput.value;\n",
      "\n",
      "        if (question) {\n",
      "            appendQuestion(question);\n",
      "\n",
      "            // make the request to the chatbot API\n",
      "            axios.post(url, {\n",
      "                question: question\n",
      "            })\n",
      "            .then(function(response) {\n",
      "                appendAnswer(response.data.answer['answer'], false);\n",
      "                scrollToBottom();\n",
      "            })\n",
      "            .catch(function(error) {\n",
      "                console.log(error);\n",
      "                // better error handling shoud be addressed in a production ready application\n",
      "            });\n",
      "\n",
      "            userInput.value = '';\n",
      "            scrollToBottom();\n",
      "        }\n",
      "    });\n",
      "\n",
      "    userInput.addEventListener('keypress', onInputKeypress, false);\n",
      "\n",
      "})(Chatbot, axios);\n",
      "```\n",
      "\n",
      "## Built With\n",
      "\n",
      "- Javascript\n",
      "- Html/CSS\n",
      "- Flask\n",
      "- Tensorflow\n",
      "- Keras\n",
      "- NLTK\n",
      "- TF-IDF\n",
      "- Scikit learn\n",
      "- Pandas\n",
      "- Numpy\n",
      "\n",
      "## Contributors\n",
      "\n",
      "üë§ **JP Santos**\n",
      "\n",
      "- Github: [@githubhandle](https://github.com/jPsantos-dev)\n",
      "- Linkedin: [linkedin](https://www.linkedin.com/in/jefferson-santos-90a327101/)\n",
      "- Email: jpsantos.dev@gmail.com\n",
      "\n",
      "## Acknowledgements\n",
      "- Hat tip to anyone whose code was used\n",
      "- Inspiration\n",
      "- etc\n",
      "=======\n",
      "# chatbot-classifier\n",
      ">>>>>>> 9f0ef0e3e8492ea480fb5ac7cec0b24d0e1efe0d\n",
      ". N√∫mero total de tokens:733\n",
      "\n",
      "Max Tokens 4000 (m√°ximo): !'})\n",
      "+\n",
      "+    if kwargs['width'] == 0:\n",
      "+        raise ValueError({'largura': 'N√£o pode ser 0'})\n",
      "+    \n",
      "+    if kwargs['height'] == 0:\n",
      "+        raise ValueError({'altura': 'N√£o pode ser 0'})\n",
      "+\n",
      "+    __metodo = _METODOS[kwargs['shape']]\n",
      "+    return round(__metodo(kwargs['width'], kwargs['height']), 2)\n",
      "+\n",
      "+\n",
      "+def main() -> None:\n",
      "+\n",
      "+    try:\n",
      "+        forms = [\n",
      "+            (60, 30, 'triangulo'),\n",
      "+            (2, 1, 'quadrado'),\n",
      "+            (0, 5, 'retangulo'),\n",
      "+            (0, 0, 'retangulo'),\n",
      "+            (12.5, 3.0, 'circulo'),\n",
      "+            (5, 2, 'teste')\n",
      "+        ]\n",
      "+\n",
      "+        for form in forms:\n",
      "+            print(get_area(*form))\n",
      "+\n",
      "+    except ValueError as e:\n",
      "+        print(*[f'Error: {k} -> {v}' for k, v in e.args[0].items()])\n",
      "+\n",
      "+\n",
      "+if __name__ == \"__main__\":\n",
      "+    main()\n",
      "+    \n",
      "+# Sa√≠da:\n",
      "+# 900.0\n",
      "+# 4\n",
      "+# Error: largura -> N√£o pode ser 0\n",
      "+# Error: largura -> N√£o pode ser 0 Error: altura -> N√£o pode ser 0\n",
      "+# 117.81\n",
      "+# Error: shape -> Formato inv√°lido! Me diga como calcular a √°rea de um c√≠rculo!\n",
      "+\n",
      "+```\n",
      "+</details>\n",
      "+\n",
      "+---\n",
      "+\n",
      "+Por enquanto √© s√≥, pessoal! Espero que tenham gostado.\n",
      "+\n",
      "+At√© a pr√≥xima! ;)<!--truncate-->\n",
      ". N√∫mero total de tokens:389\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Me diga como calcular a √°rea de um c√≠rculo\"\n",
    "\n",
    "short_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "medium_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "long_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "maximum_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "print(f\"Max Tokens 10: {short_response['choices'][0]['text']}. N√∫mero total de tokens:{short_response['usage']['total_tokens']}\\n\")\n",
    "print(f\"Max Tokens 100: {medium_response['choices'][0]['text']}. N√∫mero total de tokens:{medium_response['usage']['total_tokens']}\\n\")\n",
    "print(f\"Max Tokens 1000: {long_response['choices'][0]['text']}. N√∫mero total de tokens:{long_response['usage']['total_tokens']}\\n\")\n",
    "print(f\"Max Tokens 4000 (m√°ximo): {maximum_response['choices'][0]['text']}. N√∫mero total de tokens:{maximum_response['usage']['total_tokens']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Sequences\n",
    "\n",
    "Faz com que as respostas parem em um ponto desejado, como o fim de uma senten√ßa, ou uma lista. Especifica-se at√© quatro sequ√™ncias nas quais o modelo ir√° parar de gerar tokens na resposta. O texto retornado n√£o cont√©m as sequ√™ncias de parada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7gjhlfzw6OlBkN6HM9J32PTpJx8JD at 0x2b2002edee0> JSON: {\n",
       "  \"id\": \"cmpl-7gjhlfzw6OlBkN6HM9J32PTpJx8JD\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1690419821,\n",
       "  \"model\": \"gpt-35-turbo\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \" Claro, sem problemas. Voc\\u00ea est\\u00e1 se referindo para uma inscri\\u00e7\\u00e3o espec\\u00edfica ou a todas elas?\\n\\n\",\n",
       "      \"index\": 0,\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"logprobs\": null\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 24,\n",
       "    \"prompt_tokens\": 90,\n",
       "    \"total_tokens\": 114\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt=\"\"\"O di√°logo a seguir √© uma conversa√ß√£o com um assistente de Intelig√™ncia Artificial. O assistente √© prestativo, criativo, esperto e muito amig√°vel.\n",
    "\\n \\nHuman: Ol√°, quem √© voc√™?\n",
    "\\nAI: Ol√°, eu sou um assistente inteligente. Estou aqui pra ajud√°-lo com algo que voc√™ precise.\n",
    "\\nHuman: Eu gostaria de cancelar minha inscri√ß√£o.\n",
    "\\nAI:\"\"\"\n",
    "stop_sequences=[\"Human:\",\"AI:\"]\n",
    "\n",
    "chat_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    stop=stop_sequences,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "chat_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Probabilities\n",
    "\n",
    "Similar √† temperatura, esse par√¢metro controla a aleatoriedade da resposta, mas de uma forma diferente. Abaixar o top P ir√° retringir a escolha de tokens para os tokens mais prov√°veis, ou seja, com mais chances de serem o pr√≥ximo token a completar a senten√ßa. Aumentar o top P permitir√° que o modelo escolha tokens com alta e baixa probabilidade. Pode-se ajustar a temperatura ou o top p, mas n√£o ambos. Os valores de top p podem ir de 0 at√© 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top P 0:  A probabilidade √© um ramo da matem√°tica que se dedica ao estudo dos fen√¥menos aleat√≥rios. Ela √© utilizada para se estimar a chance de ocorr√™ncia de um determinado evento em uma experi√™ncia aleat√≥ria.\n",
      "\n",
      "Qual √© a f√≥rmula da probabilidade? A f√≥rmula da probabilidade √© dada por: P(A) = n(A) / n(S), onde P(A) √© a probabilidade do evento\n",
      "\n",
      "Top P 0.5:  \n",
      "\n",
      "A probabilidade √© uma medida num√©rica que varia de 0 a 1, que tem como objetivo medir a chance de um evento ocorrer.\n",
      "\n",
      "- Quais s√£o os m√©todos cl√°ssico e frequentista para calcular probabilidade?\n",
      "\n",
      "O m√©todo cl√°ssico √© utilizado para calcular a probabilidade de eventos que possuem um espa√ßo amostral finito e equiprov√°vel. A probabilidade √© calculada dividindo-se o n√∫mero de eventos favor√°veis pelo n√∫mero total de\n",
      "\n",
      "Top P 1: }{A probabilidade √© um conceito t√£o complexo que pode ser apresentado de diversas formas. Em uma defini√ß√£o mais matem√°tica:\n",
      "\\begin{itemize}\n",
      "\\item[\\textbf{Defini√ß√£o}] Dado um espa√ßo amostral $\\Omega$, uma probabilidade √© qualquer aplica√ß√£o $P$ que associa a cada evento $A$ um n√∫mero tal que\n",
      "\\begin{enumerate}\n",
      "\\item $0\\leq P(A) \\leq 1$\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Defina o conceito de probabilidade:\"\n",
    "\n",
    "cold_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    top_p=0,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "normal_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    top_p=0.5,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "hot_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    top_p=1,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(f\"Top P 0: {cold_response['choices'][0]['text']}\\n\")\n",
    "print(f\"Top P 0.5: {normal_response['choices'][0]['text']}\\n\")\n",
    "print(f\"Top P 1: {hot_response['choices'][0]['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Penalty\n",
    "\n",
    "Reduz a chance de repetir um token proporcionalmente baseado em qu√£o frequente ele aparece no texto at√© agora. Isso diminui a chance de repetir o exatamente o mesmo texto em uma reposta. Os valores variam de 0 at√© 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Penalty 0:  Esta √© uma de nossas perguntas [‚Ä¶]\n",
      "\n",
      "Leia mais‚Ä¶\n",
      "\n",
      "O WhatsApp √© um aplicativo incr√≠vel para comunica√ß√£o pessoal e empresarial, por√©m, ainda n√£o h√° seguran√ßa de que suas informa√ß√µes [‚Ä¶]\n",
      "\n",
      "Voc√™ j√° deve ter ouvido falar sobre API. API √© uma sigla em ingl√™s que significa ‚ÄúApplication Programming Interface‚Äù. Em [‚Ä¶]\n",
      "\n",
      "Na hora de utilizar aplicativos mobile para neg√≥cios, sempre devemos ter em mente: facilidade de uso, layout amig√°vel e boa [‚Ä¶]\n",
      "\n",
      "Com a constante atualiza√ß√£o da tecnologia e a facilidade no acesso a itens de consumo, √© necess√°rio que as marcas [‚Ä¶]\n",
      "\n",
      "Os usu√°rios de aplicativos m√≥veis est√£o aumentando a cada dia. Estima-se que haja cerca de 2 bilh√µes de usu√°rios de [‚Ä¶]\n",
      "\n",
      "Quando se trata de neg√≥cios e relacionamento com o cliente, a seguran√ßa do aplicativo √© crucial. Se voc√™ estiver procurando [‚Ä¶]\n",
      "\n",
      "Frequency Penalty 1:  As solu√ß√µes ainda dependem em grande parte do uso de an√°lises e dados dispon√≠veis para aprimorar suas capacidades.\n",
      "\n",
      "Acer inovou com projetor 4K hologr√°fico. Voc√™ j√° conhece?\n",
      "\n",
      "Live Commerce: como o Live Marketing pode ajudar a alavancar as suas vendas\n",
      "\n",
      "\n",
      "Frequency Penalty 2:  Antes de respondermos a essas perguntas, que tal mostrar como ele pode ser aplicado no dia-a-dia das empresas? Quais s√£o os benef√≠cios ? Atendimento imediato 24h por meio das ferramentas em diversas plataformas (site, Facebook leia mais‚Ä¶\n",
      "\n",
      "Cada vez mais o comportamento do cliente vem mudando. Isso se deve principalmente ao fato de termos acesso cada vez maior √† informa√ß√£o e aos produtos e servi√ßos oferecidos pelas empresas. O resultado √© um mercado cada vez mais competitivo exigindo dos empres√°rios uma maior aten√ß√£o ao p√∫blico-alvo da marca.\n",
      "\n",
      "Com isso as formas de marketing convencional v√™m perdendo a efetividade nesse cen√°rio atual. √â necess√°rio inova√ß√£o e essa inova√ß√£o come√ßa com entender qual √© o comportamento do consumidor moderno.\n",
      "\n",
      "Voc√™ sabe quem √© o seu cliente?\n",
      "\n",
      "Criar produtos ou servi√ßos adequados exige muito est\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"O que √© preciso para desenvolver um chatbot e como eu posso fazer isso?\"\n",
    "\n",
    "low_freq_penal_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    frequency_penalty=0,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "normal_freq_penal_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    frequency_penalty=1,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "high_freq_penal_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    frequency_penalty=2,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(f\"Frequency Penalty 0: {low_freq_penal_response['choices'][0]['text']}\\n\")\n",
    "print(f\"Frequency Penalty 1: {normal_freq_penal_response['choices'][0]['text']}\\n\")\n",
    "print(f\"Frequency Penalty 2: {high_freq_penal_response['choices'][0]['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presence Penalty\n",
    "\n",
    "Reduz a chance de repetir qualquer token que apareceu no texto. Isso aumenta a probabilidade de introduzir novos t√≥picos em uma resposta. Os valores variam de 0 at√© 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presence Penalty 0:  Como ele possuir√° a capacidade de dialogar com o usu√°rio, qual tipo de intelig√™ncia artificial eu posso utilizar?\n",
      "\n",
      "A partir do seu conhecimento no desenvolvimento de software, voc√™ pode facilmente construir um ChatBot chamado TesteBot, usando Node.JS e um servi√ßo do Watson da IBM com a tecnologia de processamento de linguagem natural chamada de Conversation.\n",
      "\n",
      "Seguindo os princ√≠pios do JavaScript e da arquitetura REST, Node.JS √© uma plataforma para desenvolvimento de aplica√ß√µes web escal√°veis e de alta performance, que roda na m√°quina onde est√° o servidor, geralmente, com o conjunto de servi√ßos de banco de dados.\n",
      "\n",
      "A proposta da IBM com o Conversational ajuda a entender mensagens dos usu√°rios e permitir que o chatbot reflita sobre padr√µes de fala, tendo a habilidade de entender e gerir um di√°logo, gerar questionamentos para a√ß√£o futura\n",
      "\n",
      "Presence Penalty 1:  Preciso ter conhecimentos em programa√ß√£o?‚Äù.\n",
      "\n",
      "A Bolha de Sab√£o Comunica√ß√£o Inteligente √© conhecedora dos melhores caminhos para produ√ß√£o, e lan√ßamento do seu chatbot. Fale conosco e podemos te ajudar a inserir-se no mundo da Intelig√™ncia Artificial, ou ainda como os nossos ‚ÄúDummys‚Äù muto sabiamente apontaram, no Mundo do Terceiro Setor-Consciente. Essa nova era para seus usu√°rios, se apresenta como uma nova forma de comunica√ß√£o, assist√™ncia ao cliente e dissemina√ß√£o de informa√ß√µes importantes para suas diferentes causas sociais.\n",
      "\n",
      "Nesse sentido, as empresas que investirem na plataforma ter√£o resultados significativos em rela√ß√£o a engajamento e fideliza√ß√£o de p√∫blico, al√©m de insights que poder√£o influenciar o futuro de sua organiza√ß√£o.\n",
      "\n",
      "O futuro dos chatbots √© mais promissor ainda. Eles ser√£o capazes de realizar\n",
      "\n",
      "Presence Penalty 2:  O Chatbot √© uma tecnologia de comunica√ß√£o que utiliza Intelig√™ncia Artificial para entender o que as pessoas querem e fornecer respostas personalizadas a elas. Ele se conecta √† internet pela nuvem, podendo ser programado para atender um grande n√∫mero de usu√°rios ao mesmo tempo ‚Äì sem precisar esperar em filas ou receber um grande volume de mensagens por minuto, agindo muito mais r√°pido do que qualquer operador humano.\n",
      "\n",
      "Com uma interface interativa e personalizada, dialogando com os visitantes de seu site, seu chatbot pode: responder pelo nome do usu√°rio, fazer perguntas pr√©-programadas, coletar informa√ß√µes pessoais (nome, email, telefone, interesse), enviar respostas personalizadas, e at√© √†s vezes realizar negocia√ß√µes comerciais sozinho.\n",
      "\n",
      "Voc√™ precisa ter alguns conhecimentos t√©cnicos b√°sicos sobre algoritmos e intelig√™ncia artificial mas na maior parte das vezes voc√™ contar√°\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"O que √© preciso para desenvolver um chatbot e como eu posso fazer isso?\"\n",
    "\n",
    "low_pres_penal_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    presence_penalty=0,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "normal_pres_penal_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    presence_penalty=1,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "high_pres_penal_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=prompt,\n",
    "    presence_penalty=2,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(f\"Presence Penalty 0: {low_pres_penal_response['choices'][0]['text']}\\n\")\n",
    "print(f\"Presence Penalty 1: {normal_pres_penal_response['choices'][0]['text']}\\n\")\n",
    "print(f\"Presence Penalty 2: {high_pres_penal_response['choices'][0]['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre e Pos-response text\n",
    "\n",
    "O pre-response text √© inserido depois da entrada do usu√°rio e antes da resposta do modelo. Isso pode preparar o modelo para uma resposta. Por sua vez, o pos-response text √© inserido depois da resposta gerada pelo modelo, levando o usu√°rio a oferecer uma entrada em seguinda, da forma que se faz em uma conversa√ß√£o."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
