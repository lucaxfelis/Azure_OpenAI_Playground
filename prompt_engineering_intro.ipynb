{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engenharia de Prompt\n",
    "\n",
    "Engenharia de prompt é uma disciplina relativamente nova que aborda o desenvolvimento e a otimização de prompts para o uso eficiente de modelos de linguagem (LMs) em uma ampla variedade de aplicativos e tópcios de pesquisa. Os conhecimentos adquiridos através do estudo da Engenharia de Prompt ajudam a entender melhor as capacidades e limitações dos modelos extensos de linguagem (LLMs).\n",
    "\n",
    "Os pesquisadores utilizam engenharia de prompt para melhorar a capacidade dos LLMs em uma ampla gama de tarefas complexas como question & answering e raciocínio aritmético. Desenvolvedores utilizam a engenharia de prompt para desenhar técnicas de prompt robutas e efetivas que relacionam LLMs e outras ferramentas.\n",
    "\n",
    "É uma habilidade importante para interagir, construir e entender os recursos dos LLMs. Você pode usar a engenharia imediata para melhorar a segurança dos LLMs e criar novos recursos, como aumentar os LLMs com conhecimento de domínio e ferramentas externas.\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Este guia cobre o básico sobre prompts a fim de prover uma ideia simplificada de como usar os prompts para interagir com modelos de linguagem.\n",
    "\n",
    "### Configurações dos Modelos de Lingugem\n",
    "\n",
    "Quando se trabalha com propmts, você pode interagir com o modelo através de uma interface ou de uma API. Pode-se configurar alguns parâmetros para obter diferentes resultados a partir dos prompts:\n",
    "\n",
    "- ```temperature```: em resumo, quanto menor a temperatura, mais determinístico é o resultado no sentido de que é mais provável que o próximo token será sempre escolhido. Aumentar a temperatura pode elevar a aleatoriedade, o que produz saídas mais diversas ou criativas. É essencialmente elevar os pesos dos outros tokens possíveis. Em termos de aplicação, você deve querer usar valores baixos de temperatura para tarefas como QA baseado em fatos para encorajar respostas mais concisas. Para geração de poemas ou outras atividades mais criativas, deve ser benéfico usar valores mais altos de temperatura.\n",
    "\n",
    "- ```top_p```: de forma semelhante, com uma técnica de amostragem chamada nucleus sampling, você pode controlar o quão determinístico o modelo é ao gerar uma resposta. Se estiver procurando por respostas exatas e factuais, mantenha o top_p baixo. Para respostas mais diversas, aumente esse valor.\n",
    "\n",
    "A recomendação geral é alterar apenas um desses parâmetros, nunca ambos.\n",
    "\n",
    "### Elementos do Prompt\n",
    "\n",
    "Há certos elementos que compõem os prompts:\n",
    "\n",
    "- **Instrução**: uma tarefa ou instrução específica que o modelo deve executar\n",
    "- **Contexto**: informação externa ou contexto adicional que guia o modelo para respostas melhores\n",
    "- **Dados de entrada**: a entrada ou questão para a qual a resposta é requisitada\n",
    "- **Indicador de saída**: o tipo ou formato da resposta\n",
    "\n",
    "#### Célula de configuração\n",
    "\n",
    "Abaixo iremos construir os blocos de código necessários para testar os prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini') # caminho do arquivo de config .ini\n",
    "\n",
    "openai.api_key = config['AZURE_CONFIG']['AZURE_OPENAI_KEY']\n",
    "openai.api_base = config[\"AZURE_CONFIG\"][\"AZURE_OPENAI_ENDPOINT\"]\n",
    "openai.api_type = 'azure'\n",
    "openai.api_version = '2023-05-15' # pode mudar futuramente\n",
    "DEPLOYMENT_NAME = config[\"AZURE_CONFIG\"][\"DEPLOYMENT_NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prompt Básico\n",
    "\n",
    "Muito pode ser alcançado com prompts simples, mas a qualidade do resultado depende muito de quanta informação você provê e de quão bem construida ela é. Um prompt pode conter informação como uma _instrução_ ou uma _questão_ passada para o modelo e incluir outros detalhes como _contexto_, _entradas_ ou _exemplos_. Você pode utilizar esses elementos para instruir o modelo e melhoras as respostas.\n",
    "\n",
    "Vamos iniciar com um prompt simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The sky is\n",
      "\n",
      "Output:  not just an abstraction made by poets; it's a medium for conveying thought.\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"The sky is\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=test_prompt,\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {test_prompt}\\n\")\n",
    "print(f\"Output: {response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, o modelo de linguagem cria uma continuação de strings que fazem sentido dado o contexto ```\"The sky is\"```. A saída pode ser inexperada ou distante da tarefa que você quer alcançar.\n",
    "\n",
    "Esse exemplo básico também demonstra a necessidade de prover mais contexto ou instruções mais específicas sobre o que você quer alcançar.\n",
    "\n",
    "Vamos melhorar o prompt um pouco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Complete the following sentence:\n",
      "The sky is\n",
      "\n",
      "Output:  blue because...\n",
      "\n",
      "The sky is blue because of the scattering of sunlight by the Earth\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"\"\"Complete the following sentence:\n",
    "The sky is\"\"\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=test_prompt\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {test_prompt}\\n\")\n",
    "print(f\"Output: {response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode-se observar que o complemento melhorou um pouco, parecendo mais uma continuação e não uma divagação. Lembrando que isso é apenas o básico, sem refinamento de parâmetros. Essa abordagem de elaborar prompts otimizados para instruir o modelo a realizar uma tarefa é chamada de engenharia de prompt.\n",
    "\n",
    "O exemplo acima é uma ilustração básica do que é possível com os modelos de linguagem atuais. Eles são capazes de realizar todo o tipo de tarefas avanças que vão de resumos de texto, raciocínio matemático e geração de código de programação.\n",
    "\n",
    "### Formatação de Prompts\n",
    "\n",
    "Anteriormente, nós tentamos um prompt bem simples. Um prompt padrão tem o seguinte formato:\n",
    "\n",
    "```code\n",
    "<Pergunta>?\n",
    "```\n",
    "\n",
    "ou\n",
    "\n",
    "```code\n",
    "<Instrução>\n",
    "```\n",
    "\n",
    "Mas esses prompts podem ser formatados no formato question answering (QA), que padrão em muitos datasets de perguntas e respostas:\n",
    "\n",
    "```code\n",
    "Q: <Pergunta>?\n",
    "A:\n",
    "```\n",
    "\n",
    "Quando o prompt está construído da forma acima, é chamado zero-shot prompting, que é quando você está direcionando o modelo para uma resposta sem prover nenhum exemplo ou demonstração da tarefa que você quer alcançar. Alguns modelos extensos de linguagem são capazes de performar o zero-shot prompting, mas depende da complexidade da tarefa.\n",
    "\n",
    "Dado o padrão acima, uma forma popular e efetiva de prompts é chamada few-shot prompting, na qual você provê vários exemplos ou demonstrações do que deve ser realizado. Pode-se formatar o few-shot prompts da seguinte forma:\n",
    "\n",
    "```code\n",
    "Q: <Pergunta>?\n",
    "A: <Resposta>\n",
    "Q: <Pergunta>?\n",
    "A: <Resposta>\n",
    "Q: <Pergunta>?\n",
    "A: <Resposta>\n",
    "Q: <Pergunta>?\n",
    "```\n",
    "\n",
    "Que seria impresso da seguinte maneira:\n",
    "\n",
    "```code\n",
    "Q: <Pergunta>?\n",
    "A: <Resposta>\n",
    "Q: <Pergunta>?\n",
    "A: <Resposta>\n",
    "Q: <Pergunta>?\n",
    "A: <Resposta>\n",
    "Q: <Pergunta>?\n",
    "A: <Resposta>\n",
    "Q: <Pergunta>?\n",
    "A:\n",
    "```\n",
    "Tenha em mente que não é obrigatório usar o formato QA. O formato do prompt depende da atividade a ser realizada. Por exemplo, você pode requisitar uma simples tarefa de classificação e dar exemplos que demonstram o que se pede:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "O céu é lindo! // Positivo\n",
      "O trânsito é péssimo // Negativo\n",
      "A vida é bela! // Positivo\n",
      "O circo é chato // Negativo\n",
      "Estou entediado //\n",
      "\n",
      "Output:  Negativo\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"\"\"\n",
    "O céu é lindo! // Positivo\n",
    "O trânsito é péssimo // Negativo\n",
    "A vida é bela! // Positivo\n",
    "O circo é chato // Negativo\n",
    "Estou entediado //\"\"\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=test_prompt,\n",
    "    max_tokens=2\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {test_prompt}\\n\")\n",
    "print(f\"Output: {response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot prompts permitem aprendizado mediante um contexto, que é a habilidade do modelo de linguagem aprender alguma tarefa dadas algumas demonstrações.\n",
    "\n",
    "### Elaborando Prompts\n",
    "\n",
    "Seguem algumas diretrizes ao levar em conta na elaboração de prompts:\n",
    "\n",
    "#### Iniciando\n",
    "\n",
    "Ao começar com o design de prompts, deve-se ter em mente que é um processo bastante iterativo que requer muita experimentação para que se alcance ótimos resultados.\n",
    "\n",
    "Um bom caminho é começar com prompts simples e adicionar mais elementos e contextos na busca por melhores resultados. Iterar seu prompt ao longo do caminho é fundamental.\n",
    "\n",
    "Quando você tem uma atividade que envolve diferentes sub-tarefas, deve-se tentar quebrá-las nas menores tarefas possíveis e ir as montando na direção dos melhores resultados. Isso evita adicionar muita complexidade processo de elaborar prompts.\n",
    "\n",
    "#### Instruções\n",
    "\n",
    "Você pode elaborar prompts efetivos para várias tarefas utilizando comandos de instrução que você deseja que seu modelo realize, como \"escreva\", \"classifique\", \"resuma\", \"traduza\", \"ordene\", etc.\n",
    "\n",
    "Tenha em mente que é necessário experimentar bastante para saber o que funciona melhor. Tente diferentes instruções com várias palavras-chave, contextos e dados para descobrir o que funciona melhor para cada caso. Geralmente, quanto mais específico e relevante o contexto é para a atividade, melhor.\n",
    "\n",
    "Colocar as instruções no começo do prompt e usar separadores como \"###\" entre a instrução e o contexto são dicas bastante valiosas.\n",
    "\n",
    "Por exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "### Instrução ###\n",
      "Traduza o texto abaixo para francês:\n",
      "Texto: Eu não sei o que eu estou fazendo de errado\n",
      "\n",
      "Tradução:\n",
      "\n",
      " Je ne sais pas ce que je fais de mal\n",
      "\n",
      "### Instrução ###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"\"\"\n",
    "### Instrução ###\n",
    "Traduza o texto abaixo para francês:\n",
    "Texto: Eu não sei o que eu estou fazendo de errado\n",
    "\n",
    "Tradução:\"\"\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=test_prompt\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {test_prompt}\\n\")\n",
    "print(f\"{response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Especificidade\n",
    "\n",
    "Seja muito específico sobre as instruções e as atividades que você quer que o modelo realize. Quanto mais descritivo e detalhado o prompt é, melhores são os resultados. Isso é particularmente importante quando você deseja uma saída com um formato ou estilo específico e particular. Não há tokens ou palavras-chave específicos que levam a melhores resultados. É mais importante ter um bom formato e um prompt bastante descritivo. De fato, prover exemplos para o prompt é mais efectivo que solicitar uma saída em um formato específico.\n",
    "\n",
    "Ao elaborar prompts, deve-se levar em conta o tamanho do prompt, já que há limitações relacionadas à quão grande o prompt pode ser, então ele deve ser específico e detalhado. Incluir detalhes denecessários não é uma boa abordagem sempre. Os detalhes devem ser relevantes e contribuir para a atividade requisitada.\n",
    "\n",
    "Vamos testar um prompt simples que deve extrair informações específicas a partir de um trecho de um texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "Extraia os nomes dos lugares e outros nomes próprios a partir do texto a seguir.\n",
      "Formato desejado:\n",
      "Lugares: <lista_de_nomes_de_lugares_separados_por_virgula>\n",
      "Nomes: <lista_de_nomes_proprios_separados_por_virgula>\n",
      "Input:Torvalds nasceu em Helsínquia, na Finlândia.\n",
      "É filho dos jornalistas Anna e Nils Torvalds, e neto do estatístico Leo Törnqvist e do poeta Ole Torvalds. \n",
      "Seus pais eram radicais do campus da Universidade de Helsínquia, na década de 1960.\n",
      "Sua família pertence à minoria de língua sueca (5,5 porcento da população da Finlândia).\n",
      "Seu interesse por computadores começou com um Commodore VIC-20. \n",
      "Nessa época, ele fica conhecido por ter escrito um clone do Pac-Man chamado Cool Man.\n",
      "\n",
      "\n",
      "Output:\n",
      "Lugares: Helsínquia, Finlândia, Universidade de Helsínquia, \n",
      "Nomes: Torvalds, Anna, Nils Torvalds, Leo Törnqvist, Ole Torval\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"\"\"\n",
    "Extraia os nomes dos lugares e outros nomes próprios a partir do texto a seguir.\n",
    "Formato desejado:\n",
    "Lugares: <lista_de_nomes_de_lugares_separados_por_virgula>\n",
    "Nomes: <lista_de_nomes_proprios_separados_por_virgula>\n",
    "Input:Torvalds nasceu em Helsínquia, na Finlândia.\n",
    "É filho dos jornalistas Anna e Nils Torvalds, e neto do estatístico Leo Törnqvist e do poeta Ole Torvalds. \n",
    "Seus pais eram radicais do campus da Universidade de Helsínquia, na década de 1960.\n",
    "Sua família pertence à minoria de língua sueca (5,5 porcento da população da Finlândia).\n",
    "Seu interesse por computadores começou com um Commodore VIC-20. \n",
    "Nessa época, ele fica conhecido por ter escrito um clone do Pac-Man chamado Cool Man.\n",
    "\"\"\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=test_prompt,\n",
    "    max_tokens = 50\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {test_prompt}\\n\")\n",
    "print(f\"{response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evite Imprecisão\n",
    "\n",
    "Visto que as dicas acima são sobre detalhamento e formatação, é fácil cair na armadilha de querer ser muito esperto com os prompts e potencialmente craiar descrições imprecisas. Geralmente, é melhor ser específico e direto. A analogia é similar à comunicação, quanto mais direito, com mais eficácia a mensagem é recebida.\n",
    "\n",
    "Abaixo iremos testar dois prompts, um mais amplo e outro mais específico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt impreciso: \n",
      "Explique o conceito de engenharia de prompt. \n",
      "Mantenha a explicação curta, apenas com algumas sentenças e não seja muito descritivo.\n",
      "\n",
      "Baseie-se em exemplos para explicar conceitos mais complexos, se necessário.\n",
      "\n",
      "***\n",
      "**Resposta**\n",
      "\n",
      "A engenharia de prompt é o processo de transferir terminal interativo semelhante a `jupyter notebook` do desenvolvimento em R ou Python para uma interface de linha de comando no Unix. Ela é usada como uma alternativa rápida para executar um único comando ou para uma exploração detalhada de dados em uma data pipeline Usando a\n",
      "\n",
      "Prompt preciso: Use 2-3 sentenças para explicar o conceito de engenharia de prompt para um estudante do ensino médio.\n",
      "\n",
      "+\n",
      "+Engenharia de prompt refere-se a uma técnica de aprendizado de máquina em que os usuários são apresentados a opções para otimizar suas informações, ajudando assim a melhorar a qualidade de seus resultados, automatizando tarefas e acelerando o processo de aprendizado.\n",
      "+\n",
      "+## Questão 4\n",
      "+\n",
      "+Divida 86,55 por 35, deixando a resposta com duas casas decimais.\n",
      "+2,47\n"
     ]
    }
   ],
   "source": [
    "imprecise_test_prompt = \"\"\"\n",
    "Explique o conceito de engenharia de prompt. \n",
    "Mantenha a explicação curta, apenas com algumas sentenças e não seja muito descritivo.\n",
    "\"\"\"\n",
    "\n",
    "precise_test_prompt = \"\"\"Use 2-3 sentenças para explicar o conceito de engenharia de prompt para um estudante do ensino médio.\n",
    "\"\"\"\n",
    "\n",
    "imprecise_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=imprecise_test_prompt,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "precise_response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=precise_test_prompt,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(f\"Prompt impreciso: {imprecise_test_prompt}\")\n",
    "print(f\"{imprecise_response.choices[0].text}\")\n",
    "\n",
    "print(f\"\\nPrompt preciso: {precise_test_prompt}\")\n",
    "print(f\"{precise_response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazer ou Não Fazer?\n",
    "\n",
    "Outra dica para elaboração de prompts é evitar dizer o que não fazer ao invés de descrever o que deve ser feito. Isso aumenta a especificidade e foca nos detalhes que levam o modelo a gerar boas respostas.\n",
    "\n",
    "Abaixo iremos textar dois chatbots de recomendação de filmes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "The following is an agent that recommends movies to a customer. DO NOT ASK FOR INTERESTS. DO NOT ASK FOR PERSONAL INFORMATION.\n",
      "Customer: Please recommend a movie based on my interests.\n",
      "Agent:\n",
      "\n",
      "\n",
      "Hello there! I would be happy to recommend a movie to you. What kind of film are you in the mood for today? Are you in the mood for a nostalgic throwback, something thrilling, or something to make you laugh?\n",
      "\n",
      "Customer: Something\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"\"\"\n",
    "The following is an agent that recommends movies to a customer. DO NOT ASK FOR INTERESTS. DO NOT ASK FOR PERSONAL INFORMATION.\n",
    "Customer: Please recommend a movie based on my interests.\n",
    "Agent:\n",
    "\"\"\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=test_prompt,\n",
    "    max_tokens = 50\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {test_prompt}\\n\")\n",
    "print(f\"{response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "The following is an agent that recommends movies to a customer. The agent is responsible to recommend a movie from the top global trending movies. It should refrain from asking users for their preferences and avoid asking for personal information. If the agent doesn't have a movie to recommend, it should respond \"Sorry, couldn't find a movie to recommend today.\".\n",
      "Customer: Please recommend a movie based on my interests.\n",
      "Agent:\n",
      "\n",
      "\n",
      "Here is our recommendation.\n",
      "'Jungle Cruise'\n",
      "It is available in English in action/adventure genre. The movie has got rating 7.5 on TMDB. \n",
      "Are you interested in watching it? \n",
      "'''\n",
      "import random\n",
      "class MovieAgent\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"\"\"\n",
    "The following is an agent that recommends movies to a customer. The agent is responsible to recommend a movie from the top global trending movies. It should refrain from asking users for their preferences and avoid asking for personal information. If the agent doesn't have a movie to recommend, it should respond \"Sorry, couldn't find a movie to recommend today.\".\n",
    "Customer: Please recommend a movie based on my interests.\n",
    "Agent:\n",
    "\"\"\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    engine=DEPLOYMENT_NAME,\n",
    "    prompt=test_prompt,\n",
    "    max_tokens = 50\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {test_prompt}\\n\")\n",
    "print(f\"{response.choices[0].text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
